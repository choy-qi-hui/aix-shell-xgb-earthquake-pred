{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2IAxCqYpppCG",
      "metadata": {
        "id": "2IAxCqYpppCG"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52316068",
      "metadata": {
        "id": "52316068"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine learning\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Geospatial\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Polygon\n",
        "from shapely import wkt\n",
        "import contextily as ctx  # for background basemap tiles\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import folium\n",
        "import branca.colormap as cm\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Explainable AI\n",
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6616d20",
      "metadata": {
        "id": "c6616d20"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe11558a",
      "metadata": {
        "id": "fe11558a"
      },
      "source": [
        "### Prepare earthquake data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6bad838",
      "metadata": {
        "id": "d6bad838"
      },
      "outputs": [],
      "source": [
        "col_names = ['Date', 'Time', 'Event Type', 'GT', 'Magnitude', 'Magnitude Type', 'Latitude', 'Longitude', 'Depth', 'Quality', 'Event ID', 'Number of Picked Phases', 'Ngrams']\n",
        "\n",
        "all_eq = []\n",
        "\n",
        "for year in range(1977, 2019): # Exclude years not in wells data\n",
        "    filename = f'data/raw/SCEC_DC/{year}.catalog'\n",
        "\n",
        "    eq = pd.read_csv(filename, sep=r'\\s+', comment='#', header=None, names=col_names)\n",
        "\n",
        "    eq['Datetime'] = pd.to_datetime(eq['Date'] + ' ' + eq['Time'], errors='coerce') # Invalid entries set as NaT\n",
        "    eq['Year'] = year\n",
        "\n",
        "    all_eq.append(eq)\n",
        "\n",
        "eqs = pd.concat(all_eq, ignore_index=True)\n",
        "eqs.to_csv('data/processed/earthquakes.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d321855",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5d321855",
        "outputId": "3b14aaf2-ab22-4f92-e2a5-0ce0350246df"
      },
      "outputs": [],
      "source": [
        "eqs = pd.read_csv('data/processed/earthquakes.csv')\n",
        "eqs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31781ef4",
      "metadata": {
        "id": "31781ef4"
      },
      "source": [
        "### Prepare well injections data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "908d09da",
      "metadata": {
        "id": "908d09da",
        "outputId": "2fb36c8e-9602-4014-c260-9ebe167d2aa0"
      },
      "outputs": [],
      "source": [
        "# Obtain the API numbers to loop through\n",
        "api_filepath = 'data/raw/dataAllFields/WellsAPInumber.dat'\n",
        "\n",
        "with open(api_filepath, 'r') as f:\n",
        "    wells_api = [line.strip() for line in f]\n",
        "\n",
        "all_injs = []\n",
        "\n",
        "for api in wells_api:\n",
        "    filepath = f'data/raw/dataAllFields/Well_Injection_API_{api}.xlsx'\n",
        "    well_metadata = pd.read_excel(filepath, nrows=1)\n",
        "    injection_data = pd.read_excel(filepath, skiprows=3)\n",
        "\n",
        "    # Drop columns in injection_data containing 'Unnamed'\n",
        "    drop_cols = injection_data.columns[injection_data.columns.str.contains('Unnamed')]\n",
        "    injection_data.drop(drop_cols, axis=1, inplace=True)\n",
        "\n",
        "    # Handle case when there are no rows in injection_data - set all column values as NA\n",
        "    if injection_data.empty:\n",
        "        injection_data = pd.DataFrame({col: pd.Series([pd.NA], dtype='object') for col in injection_data.columns})\n",
        "\n",
        "    # Duplicate well_metadata rows to match number of rows of injection_data\n",
        "    well_md = pd.concat([well_metadata]*len(injection_data), ignore_index=True)\n",
        "\n",
        "    # Final concatenation\n",
        "    injection = pd.concat([well_md, injection_data], axis=1)\n",
        "\n",
        "    all_injs.append(injection)\n",
        "\n",
        "injs = pd.concat(all_injs, ignore_index=True)\n",
        "\n",
        "# Convert columns into correct dtypes\n",
        "injs = injs.convert_dtypes()\n",
        "\n",
        "injs.to_csv('data/processed/well_injections.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31f9ff76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "31f9ff76",
        "outputId": "cf760377-743d-4945-da50-0b3bb328318e"
      },
      "outputs": [],
      "source": [
        "inj = pd.read_csv('data/processed/well_injections.csv', low_memory=False)\n",
        "inj.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DNUhPI5Dv8gJ",
      "metadata": {
        "id": "DNUhPI5Dv8gJ"
      },
      "source": [
        "### Prepare well production data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ipu5SHbjwA0W",
      "metadata": {
        "id": "Ipu5SHbjwA0W"
      },
      "outputs": [],
      "source": [
        "# Obtain the API numbers to loop through\n",
        "api_filepath = 'data/raw/dataAllFields/WellsAPInumber.dat'\n",
        "\n",
        "with open(api_filepath, 'r') as f:\n",
        "    wells_api = [line.strip() for line in f]\n",
        "\n",
        "all_prod = []\n",
        "\n",
        "for api in wells_api:\n",
        "    filepath = f'data/raw/dataAllFields/Well_Production_API_{api}.xlsx'\n",
        "    well_metadata = pd.read_excel(filepath, nrows=1)\n",
        "    production_data = pd.read_excel(filepath, skiprows=3)\n",
        "\n",
        "    # Drop columns in production_data containing 'Unnamed'\n",
        "    drop_cols = production_data.columns[production_data.columns.str.contains('Unnamed')]\n",
        "    production_data.drop(drop_cols, axis=1, inplace=True)\n",
        "\n",
        "    # Handle case when there are no rows in production_data - set all column values as NA\n",
        "    if production_data.empty:\n",
        "        production_data = pd.DataFrame({col: pd.Series([pd.NA], dtype='object') for col in production_data.columns})\n",
        "\n",
        "    # Duplicate well_metadata rows to match number of rows of production_data\n",
        "    well_md = pd.concat([well_metadata]*len(production_data), ignore_index=True)\n",
        "\n",
        "    # Final concatenation\n",
        "    production = pd.concat([well_md, production_data], axis=1)\n",
        "\n",
        "    all_prod.append(production)\n",
        "\n",
        "prod = pd.concat(all_prod, ignore_index=True)\n",
        "\n",
        "# Convert columns into correct dtypes\n",
        "prod = prod.convert_dtypes()\n",
        "\n",
        "prod.to_csv('data/processed/well_productions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BSa83n7zwTuz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BSa83n7zwTuz",
        "outputId": "5b095474-d75e-47ab-8c26-cb3f710c5d65"
      },
      "outputs": [],
      "source": [
        "prod = pd.read_csv('data/processed/well_productions.csv', low_memory=False)\n",
        "prod.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a352f2c",
      "metadata": {
        "id": "6a352f2c"
      },
      "source": [
        "# Plot distribution of well locations with their corresponding:   \n",
        "- Total water/steam injected  \n",
        "- Total gas/air injected  \n",
        "- Total water produced  \n",
        "- Total gas produced  \n",
        "- Total oil produced  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3afe24e",
      "metadata": {
        "id": "a3afe24e"
      },
      "source": [
        "## Total water/steam injected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qD0ZqKfm16wZ",
      "metadata": {
        "id": "qD0ZqKfm16wZ"
      },
      "source": [
        "### Prepare total injections data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C3nJhIxq-CeA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3nJhIxq-CeA",
        "outputId": "d1ceb86e-4530-4595-973b-36f4d25c6927"
      },
      "outputs": [],
      "source": [
        "inj.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb80468f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "cb80468f",
        "outputId": "93cf6b54-b2f6-4c8d-b7c1-fe78575cabf4"
      },
      "outputs": [],
      "source": [
        "# Only use rows containing annual total\n",
        "inj_total = inj[inj['Injection Date'].str.contains('Total', case=False, na=False)].copy()\n",
        "\n",
        "# Sum all annual total, grouped by 'Well #'\n",
        "inj_total['Total Water or Steam Injected (bbl)'] = inj_total.groupby('Well #')['Water or Steam Injected (bbl)'].transform('sum')\n",
        "inj_total['Total Gas or Air Injected (Mcf)'] = inj_total.groupby('Well #')['Gas or Air Injected (Mcf)'].transform('sum')\n",
        "\n",
        "# Keep only unique wells\n",
        "inj_totals = inj_total.drop_duplicates(subset='Well #', keep='first').copy()\n",
        "\n",
        "# Drop invalid coordinates - latitude 0, longitude 0\n",
        "inj_totals = inj_totals[(inj_totals['Latitude'] != 0) & (inj_totals['Longitude'] != 0)]\n",
        "\n",
        "inj_totals.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb209395",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb209395",
        "outputId": "36fd3749-bac1-43b5-ff4d-29418ee3e57c"
      },
      "outputs": [],
      "source": [
        "inj_totals.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603d8dd3",
      "metadata": {
        "id": "603d8dd3"
      },
      "outputs": [],
      "source": [
        "# Set colour scale based on total water/steam injected\n",
        "min_val = np.log1p(inj_totals['Total Water or Steam Injected (bbl)'].min())\n",
        "max_val = np.log1p(inj_totals['Total Water or Steam Injected (bbl)'].max())\n",
        "\n",
        "colormap = cm.linear.RdBu_10.scale(min_val, max_val).to_step(10)\n",
        "\n",
        "colormap.caption = 'Logarithmic Total Water/Steam Injected (bbl)'\n",
        "\n",
        "# Create base map\n",
        "wells_water_inj_map = folium.Map(location=[34.05, -118.25], zoom_start=10, tiles='OpenStreetMap')\n",
        "\n",
        "for _, row in inj_totals.iterrows():\n",
        "    vol = np.log1p(row['Total Water or Steam Injected (bbl)']) # Scale the values down\n",
        "    color = colormap(vol)\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Latitude'], row['Longitude']],\n",
        "        radius=5,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_color=color,\n",
        "        fill_opacity=1,\n",
        "        opacity=0\n",
        "    ).add_to(wells_water_inj_map)\n",
        "\n",
        "# Add legend\n",
        "colormap.add_to(wells_water_inj_map)\n",
        "\n",
        "wells_water_inj_map.save(\"vizs/distr_maps/wells_water_inj_map.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "537dd64e",
      "metadata": {
        "id": "537dd64e"
      },
      "source": [
        "## Total gas/air injected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l7Fsi4wqf2ha",
      "metadata": {
        "id": "l7Fsi4wqf2ha"
      },
      "outputs": [],
      "source": [
        "# Set colour scale based on total gas/air injected\n",
        "min_val = np.log1p(inj_totals['Total Gas or Air Injected (Mcf)'].min())\n",
        "max_val = np.log1p(inj_totals['Total Gas or Air Injected (Mcf)'].max())\n",
        "\n",
        "colormap = cm.linear.RdBu_10.scale(min_val, max_val).to_step(10)\n",
        "\n",
        "colormap.caption = 'Logarithmic Total Gas/Air Injected (Mcf)'\n",
        "\n",
        "# Create base map\n",
        "wells_gas_inj_map = folium.Map(location=[34.05, -118.25], zoom_start=10, tiles='OpenStreetMap')\n",
        "\n",
        "for _, row in inj_totals.iterrows():\n",
        "    vol = np.log1p(row['Total Gas or Air Injected (Mcf)'])\n",
        "    color = colormap(vol)\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Latitude'], row['Longitude']],\n",
        "        radius=5,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_color=color,\n",
        "        fill_opacity=1,\n",
        "        opacity=0\n",
        "    ).add_to(wells_gas_inj_map)\n",
        "\n",
        "# Add legend\n",
        "colormap.add_to(wells_gas_inj_map)\n",
        "\n",
        "wells_gas_inj_map.save(\"vizs/distr_maps/wells_gas_inj_map.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ES_SO5FA30aI",
      "metadata": {
        "id": "ES_SO5FA30aI"
      },
      "source": [
        "#### Side-by-side view of both injection maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a9350d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "95a9350d",
        "outputId": "3a771baf-649f-4035-a1aa-a024f7d24a37"
      },
      "outputs": [],
      "source": [
        "water_html = wells_water_inj_map._repr_html_()\n",
        "gas_html = wells_gas_inj_map._repr_html_()\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<div style=\"display: flex; justify-content: space-between;\">\n",
        "\n",
        "  <div style=\"width: 49.5%;\">\n",
        "    <h3 style=\"text-align: center;\">Distribution of Wells by Total Water/Steam Injected</h3>\n",
        "    {water_html}\n",
        "  </div>\n",
        "\n",
        "  <div style=\"width: 49.5%;\">\n",
        "    <h3 style=\"text-align: center;\">Distribution of Wells by Total Gas/Air Injected</h3>\n",
        "    {gas_html}\n",
        "  </div>\n",
        "\n",
        "</div>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7e497e",
      "metadata": {
        "id": "8c7e497e"
      },
      "outputs": [],
      "source": [
        "# Save maps - download both well injection html maps first before the combined, then make sure all 3 are in the same local path before opening the combined html\n",
        "with open(\"vizs/distr_maps/combined_well_inj_maps.html\", \"w\") as f:\n",
        "    f.write(f\"\"\"\n",
        "    <html>\n",
        "    <head><title>Well Injection Maps</title></head>\n",
        "    <body>\n",
        "\n",
        "    <div style=\"display: flex; justify-content: space-between;\">\n",
        "\n",
        "      <div style=\"width: 49.5%;\">\n",
        "        <h3 style=\"text-align: center;\">Distribution of Wells by Total Water/Steam Injected</h3>\n",
        "        <iframe src=\"wells_water_inj_map.html\" width=\"100%\" height=\"500\" style=\"border:none;\"></iframe>\n",
        "      </div>\n",
        "\n",
        "      <div style=\"width: 49.5%;\">\n",
        "        <h3 style=\"text-align: center;\">Distribution of Wells by Total Gas/Air Injected</h3>\n",
        "        <iframe src=\"wells_gas_inj_map.html\" width=\"100%\" height=\"500\" style=\"border:none;\"></iframe>\n",
        "      </div>\n",
        "\n",
        "    </div>\n",
        "\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "auHkM36W8-M0",
      "metadata": {
        "id": "auHkM36W8-M0"
      },
      "source": [
        "## Total water produced"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3by2Erla9NR5",
      "metadata": {
        "id": "3by2Erla9NR5"
      },
      "source": [
        "### Prepare total production data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g5Yij9Sv9i1i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5Yij9Sv9i1i",
        "outputId": "825a8228-d769-48f3-8539-1c9dcbcce854"
      },
      "outputs": [],
      "source": [
        "prod.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L-VQDunt9O3C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "L-VQDunt9O3C",
        "outputId": "a7dba75b-290a-4937-e7dd-4b1b6e11d04d"
      },
      "outputs": [],
      "source": [
        "# Only use rows containing annual total\n",
        "prod_total = prod[prod['Production Date'].str.contains('Total', case=False, na=False)].copy()\n",
        "\n",
        "# Sum all annual total, grouped by 'Well #'\n",
        "prod_total['Total Water Produced (bbl)'] = prod_total.groupby('Well #')['Water Produced (bbl)'].transform('sum')\n",
        "prod_total['Total Gas Produced (Mcf)'] = prod_total.groupby('Well #')['Gas Produced (Mcf)'].transform('sum')\n",
        "prod_total['Total Oil Produced (bbl)'] = prod_total.groupby('Well #')['Oil Produced (bbl)'].transform('sum')\n",
        "\n",
        "# Keep only unique wells\n",
        "prod_totals = prod_total.drop_duplicates(subset='Well #', keep='first').copy()\n",
        "\n",
        "# Drop invalid coordinates - latitude 0, longitude 0\n",
        "prod_totals = prod_totals[(prod_totals['Latitude'] != 0) & (prod_totals['Longitude'] != 0)]\n",
        "\n",
        "prod_totals.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eBDpeyVH9UNo",
      "metadata": {
        "id": "eBDpeyVH9UNo"
      },
      "outputs": [],
      "source": [
        "# Set colour scale based on total water produced\n",
        "min_val = np.log1p(prod_totals['Total Water Produced (bbl)'].min())\n",
        "max_val = np.log1p(prod_totals['Total Water Produced (bbl)'].max())\n",
        "\n",
        "colormap = cm.linear.RdBu_10.scale(min_val, max_val).to_step(10)\n",
        "\n",
        "colormap.caption = 'Logarithmic Total Water Produced (bbl)'\n",
        "\n",
        "# Create base map\n",
        "wells_water_prod_map = folium.Map(location=[34.05, -118.25], zoom_start=10, tiles='OpenStreetMap')\n",
        "\n",
        "for _, row in prod_totals.iterrows():\n",
        "    vol = np.log1p(row['Total Water Produced (bbl)']) # Scale the values down\n",
        "    color = colormap(vol)\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Latitude'], row['Longitude']],\n",
        "        radius=5,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_color=color,\n",
        "        fill_opacity=1,\n",
        "        opacity=0\n",
        "    ).add_to(wells_water_prod_map)\n",
        "\n",
        "# Add legend\n",
        "colormap.add_to(wells_water_prod_map)\n",
        "\n",
        "wells_water_prod_map.save(\"vizs/distr_maps/wells_water_prod_map.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UkvjNdz59Ve2",
      "metadata": {
        "id": "UkvjNdz59Ve2"
      },
      "source": [
        "## Total gas produced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5_8_XHZR9W5z",
      "metadata": {
        "id": "5_8_XHZR9W5z"
      },
      "outputs": [],
      "source": [
        "# Set colour scale based on total gas produced\n",
        "min_val = np.log1p(prod_totals['Total Gas Produced (Mcf)'].min())\n",
        "max_val = np.log1p(prod_totals['Total Gas Produced (Mcf)'].max())\n",
        "\n",
        "colormap = cm.linear.RdBu_10.scale(min_val, max_val).to_step(10)\n",
        "\n",
        "colormap.caption = 'Logarithmic Total Gas Produced (Mcf)'\n",
        "\n",
        "# Create base map\n",
        "wells_gas_prod_map = folium.Map(location=[34.05, -118.25], zoom_start=10, tiles='OpenStreetMap')\n",
        "\n",
        "for _, row in prod_totals.iterrows():\n",
        "    vol = np.log1p(row['Total Gas Produced (Mcf)']) # Scale the values down\n",
        "    color = colormap(vol)\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Latitude'], row['Longitude']],\n",
        "        radius=5,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_color=color,\n",
        "        fill_opacity=1,\n",
        "        opacity=0\n",
        "    ).add_to(wells_gas_prod_map)\n",
        "\n",
        "# Add legend\n",
        "colormap.add_to(wells_gas_prod_map)\n",
        "\n",
        "wells_gas_prod_map.save(\"vizs/distr_maps/wells_gas_prod_map.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3B7C7wFgAHS1",
      "metadata": {
        "id": "3B7C7wFgAHS1"
      },
      "source": [
        "## Total oil produced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NfzLlTImAKiB",
      "metadata": {
        "id": "NfzLlTImAKiB"
      },
      "outputs": [],
      "source": [
        "# Set colour scale based on total oil produced\n",
        "min_val = np.log1p(prod_totals['Total Oil Produced (bbl)'].min())\n",
        "max_val = np.log1p(prod_totals['Total Oil Produced (bbl)'].max())\n",
        "\n",
        "colormap = cm.linear.RdBu_10.scale(min_val, max_val).to_step(10)\n",
        "\n",
        "colormap.caption = 'Logarithmic Total Oil Produced (bbl)'\n",
        "\n",
        "# Create base map\n",
        "wells_oil_prod_map = folium.Map(location=[34.05, -118.25], zoom_start=10, tiles='OpenStreetMap')\n",
        "\n",
        "for _, row in prod_totals.iterrows():\n",
        "    vol = np.log1p(row['Total Oil Produced (bbl)']) # Scale the values down\n",
        "    color = colormap(vol)\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Latitude'], row['Longitude']],\n",
        "        radius=5,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_color=color,\n",
        "        fill_opacity=1,\n",
        "        opacity=0\n",
        "    ).add_to(wells_oil_prod_map)\n",
        "\n",
        "# Add legend\n",
        "colormap.add_to(wells_oil_prod_map)\n",
        "\n",
        "wells_oil_prod_map.save(\"vizs/distr_maps/wells_oil_prod_map.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ECpBO52b9YzX",
      "metadata": {
        "id": "ECpBO52b9YzX"
      },
      "source": [
        "#### Side-by-side view of all production maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PHRXFvbs9aiL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "PHRXFvbs9aiL",
        "outputId": "c969d653-03c8-4e95-f9c4-1b2dc9a16e3e"
      },
      "outputs": [],
      "source": [
        "water_html = wells_water_prod_map._repr_html_()\n",
        "gas_html = wells_gas_prod_map._repr_html_()\n",
        "oil_html = wells_oil_prod_map._repr_html_()\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<div style=\"display: flex; justify-content: space-between;\">\n",
        "\n",
        "  <div style=\"width: 33%;\">\n",
        "    <h3 style=\"text-align: center;\">Distribution of Wells by Total Water Produced</h3>\n",
        "    {water_html}\n",
        "  </div>\n",
        "\n",
        "  <div style=\"width: 33%;\">\n",
        "    <h3 style=\"text-align: center;\">Distribution of Wells by Total Gas Produced</h3>\n",
        "    {gas_html}\n",
        "  </div>\n",
        "\n",
        "  <div style=\"width: 33%;\">\n",
        "    <h3 style=\"text-align: center;\">Distribution of Wells by Total Oil Produced</h3>\n",
        "    {gas_html}\n",
        "  </div>\n",
        "\n",
        "</div>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N5EJLMG69ccj",
      "metadata": {
        "id": "N5EJLMG69ccj"
      },
      "outputs": [],
      "source": [
        "# Save maps - download all well production html maps first before the combined, then make sure all 4 are in the same local path before opening the combined html\n",
        "with open(\"vizs/distr_maps/combined_well_prod_maps.html\", \"w\") as f:\n",
        "    f.write(f\"\"\"\n",
        "    <html>\n",
        "    <head><title>Well Production Maps</title></head>\n",
        "    <body>\n",
        "\n",
        "    <div style=\"display: flex; justify-content: space-between;\">\n",
        "\n",
        "      <div style=\"width: 33%;\">\n",
        "        <h3 style=\"text-align: center;\">Distribution of Wells by Total Water Produced</h3>\n",
        "        <iframe src=\"wells_water_prod_map.html\" width=\"100%\" height=\"500\" style=\"border:none;\"></iframe>\n",
        "      </div>\n",
        "\n",
        "      <div style=\"width: 33%;\">\n",
        "        <h3 style=\"text-align: center;\">Distribution of Wells by Total Gas Produced</h3>\n",
        "        <iframe src=\"wells_gas_prod_map.html\" width=\"100%\" height=\"500\" style=\"border:none;\"></iframe>\n",
        "      </div>\n",
        "\n",
        "      <div style=\"width: 33%;\">\n",
        "        <h3 style=\"text-align: center;\">Distribution of Wells by Total Oil Produced</h3>\n",
        "        <iframe src=\"wells_oil_prod_map.html\" width=\"100%\" height=\"500\" style=\"border:none;\"></iframe>\n",
        "      </div>\n",
        "\n",
        "    </div>\n",
        "\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7253a9a",
      "metadata": {
        "id": "c7253a9a"
      },
      "source": [
        "# XGBoost model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RuUknFDjPrM9",
      "metadata": {
        "id": "RuUknFDjPrM9"
      },
      "source": [
        "## Further data preprocessing and feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcf9e29",
      "metadata": {
        "id": "dfcf9e29"
      },
      "outputs": [],
      "source": [
        "# Drop irrelevant columns\n",
        "drop_cols = ['API #', 'Operator Name', 'County Name', 'Field Name', 'Lease Name',\n",
        "                'Area Name', 'Area Code', 'District #', 'Section', 'Township',\n",
        "                'Range', 'Base Meridian', 'API Number','PWT Status', 'Status', 'Pool Code',\n",
        "             'Event Type', 'GT', 'Event ID', 'Number of Picked Phases', 'Ngrams']\n",
        "\n",
        "# Create a new DataFrame with those columns dropped\n",
        "inj_copy = inj.drop(columns=drop_cols, errors='ignore').copy()\n",
        "prod_copy = prod.drop(columns=drop_cols, errors='ignore').copy()\n",
        "eqs_copy = eqs.drop(columns=drop_cols, errors='ignore').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qXcFpRRjT6QN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "qXcFpRRjT6QN",
        "outputId": "e62034bd-dfbc-4eb0-c230-e504457e33d3"
      },
      "outputs": [],
      "source": [
        "# Make sure all date cols are in Datetime\n",
        "inj_copy1 = inj_copy.copy()\n",
        "prod_copy1 = prod_copy.copy()\n",
        "eqs_copy1 = eqs_copy.copy()\n",
        "\n",
        "inj_copy1['Injection Date'] = pd.to_datetime(inj_copy1['Injection Date'], errors='coerce')\n",
        "prod_copy1['Production Date'] = pd.to_datetime(prod_copy1['Production Date'], errors='coerce')\n",
        "eqs_copy1['Date'] = pd.to_datetime(eqs_copy1['Date'], errors='coerce')\n",
        "\n",
        "# Create YearMonth col for merging\n",
        "inj_copy1['YearMonth'] = inj_copy1['Injection Date'].dt.to_period('M')\n",
        "prod_copy1['YearMonth'] = prod_copy1['Production Date'].dt.to_period('M')\n",
        "eqs_copy1['YearMonth'] = eqs_copy1['Date'].dt.to_period('M')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "380f61ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "380f61ca",
        "outputId": "1af9f09f-92cb-4891-ce98-1584292cd649"
      },
      "outputs": [],
      "source": [
        "# Merge inj and prod\n",
        "inj_prod = pd.merge(inj_copy1, prod_copy1,\n",
        "                    on=['Latitude', 'Longitude', 'YearMonth'],\n",
        "                    how='inner')\n",
        "\n",
        "inj_prod.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235e7c50",
      "metadata": {
        "id": "235e7c50"
      },
      "outputs": [],
      "source": [
        "# Add Month and Geometry cols into inj_prod\n",
        "# Convert DataFrame to GeoDataFrame\n",
        "geometry = gpd.points_from_xy(inj_prod['Longitude'], inj_prod['Latitude'])\n",
        "gdf = gpd.GeoDataFrame(inj_prod, geometry=geometry, crs=\"EPSG:4326\")\n",
        "\n",
        "# Get bounding box of all points\n",
        "minx, miny, maxx, maxy = gdf.total_bounds\n",
        "\n",
        "grid_size = 0.05  # ~5km\n",
        "\n",
        "cols = list(np.arange(minx, maxx + grid_size, grid_size))\n",
        "rows = list(np.arange(miny, maxy + grid_size, grid_size))\n",
        "\n",
        "# Create polygons for the grid\n",
        "polygons = []\n",
        "ids = []\n",
        "for i, x in enumerate(cols[:-1]):\n",
        "    for j, y in enumerate(rows[:-1]):\n",
        "        polygons.append(\n",
        "            Polygon([\n",
        "                (x, y),\n",
        "                (x + grid_size, y),\n",
        "                (x + grid_size, y + grid_size),\n",
        "                (x, y + grid_size)\n",
        "            ])\n",
        "        )\n",
        "        ids.append(f'{i}_{j}')\n",
        "\n",
        "grid = gpd.GeoDataFrame({'grid_id': ids, 'grid_poly': polygons}, crs=\"EPSG:4326\", geometry=polygons)\n",
        "\n",
        "# Spatial join to attach grid polygon to each point\n",
        "inj_prod_gdf = gpd.sjoin(gdf, grid, how='inner', predicate='intersects')\n",
        "\n",
        "# Keep polygon geometry in a new column instead of replacing point geometry\n",
        "inj_prod_gdf = inj_prod_gdf.rename(columns={'grid_poly': 'grid_geometry'})\n",
        "\n",
        "# Format YearMonth string\n",
        "inj_prod_gdf['YearMonth'] = inj_prod_gdf['YearMonth'].dt.strftime('%Y-%m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c92944b7",
      "metadata": {
        "id": "c92944b7",
        "outputId": "b6554af3-f195-4be0-dd71-ddaca813c630"
      },
      "outputs": [],
      "source": [
        "inj_prod_gdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf42e42",
      "metadata": {
        "id": "bdf42e42"
      },
      "outputs": [],
      "source": [
        "# Add Month and Geometry cols into eqs_copy1, using inj_prod_gdf bounding box\n",
        "# Convert DataFrame to GeoDataFrame\n",
        "geometry = gpd.points_from_xy(eqs_copy1['Longitude'], eqs_copy1['Latitude'])\n",
        "gdf = gpd.GeoDataFrame(eqs_copy1, geometry=geometry, crs=\"EPSG:4326\")\n",
        "\n",
        "# Spatial join to attach grid polygon to each point using inj_prod_gdf's grid\n",
        "eqs_gdf = gpd.sjoin(gdf, grid, how='inner', predicate='intersects')\n",
        "\n",
        "# Keep polygon geometry in a new column instead of replacing point geometry\n",
        "eqs_gdf = eqs_gdf.rename(columns={'grid_poly': 'grid_geometry'})\n",
        "\n",
        "# Format YearMonth string\n",
        "eqs_gdf['YearMonth'] = eqs_gdf['YearMonth'].dt.strftime('%Y-%m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2466f7ea",
      "metadata": {
        "id": "2466f7ea",
        "outputId": "c14da239-1af2-4c9e-fb11-4e9c80c354fa"
      },
      "outputs": [],
      "source": [
        "eqs_gdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6053d9e7",
      "metadata": {
        "id": "6053d9e7",
        "outputId": "31419d1f-fc0f-404b-fc51-841e277a91e7"
      },
      "outputs": [],
      "source": [
        "print(inj_prod_gdf.columns)\n",
        "print(eqs_gdf.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0088edb5",
      "metadata": {
        "id": "0088edb5",
        "outputId": "05d1764e-88ca-4029-bff6-01f35fb5bcba"
      },
      "outputs": [],
      "source": [
        "# Merge injections & productions table with earthquake data\n",
        "overall = pd.merge(inj_prod_gdf, eqs_gdf,\n",
        "                   on=['grid_id', 'grid_geometry', 'YearMonth'], how='inner')\n",
        "\n",
        "overall.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c01a130",
      "metadata": {
        "id": "4c01a130",
        "outputId": "b02000d8-8496-4aca-b2ba-21a3cce71670"
      },
      "outputs": [],
      "source": [
        "overall.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766da010",
      "metadata": {
        "id": "766da010",
        "outputId": "094fe643-30a9-45af-ade8-72de1a357a6f"
      },
      "outputs": [],
      "source": [
        "# Create a copy of overall df, with more focused, fewer columns\n",
        "overall_df = overall.copy()\n",
        "drop_cols = ['Well #_x', 'Latitude_x', 'Longitude_x', 'Injection Date','Well Type_x', 'Reported Date_x',\n",
        "             'Well #_y', 'Well Type_y', 'Reported Date_y', 'geometry_x', 'index_right_x', 'Date', 'Time', \n",
        "             'Latitude_y', 'Longitude_y', 'Datetime', 'Year', 'geometry_y', 'index_right_y']\n",
        "overall_df = overall_df.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "overall_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e65ccb",
      "metadata": {},
      "outputs": [],
      "source": [
        "overall_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94908b7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "overall_df = overall_df.sort_values(['grid_id'])\n",
        "\n",
        "# Calculate lags and rolling averages\n",
        "value_cols = ['Water or Steam Injected (bbl)', 'Gas or Air Injected (Mcf)', \n",
        "              'Oil Produced (bbl)', 'Water Produced (bbl)', 'Gas Produced (Mcf)'] \n",
        "\n",
        "time_windows = {'3m': 3, '1y': 12, '5y': 60}\n",
        "\n",
        "grouped = overall_df.groupby(['grid_id'])\n",
        "\n",
        "for col in value_cols:\n",
        "    for label, window in time_windows.items():\n",
        "\n",
        "        # Lag (value from previous month)\n",
        "        overall_df[f'{col}_lag_{label}'] = grouped[col].transform(lambda x: x.shift(1))\n",
        "\n",
        "        # Rolling mean\n",
        "        overall_df[f'{col}_roll_mean_{label}'] = grouped[col].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
        "\n",
        "        # Rolling sum\n",
        "        overall_df[f'{col}_roll_sum_{label}'] = grouped[col].transform(lambda x: x.rolling(window, min_periods=1).sum())\n",
        "\n",
        "        # Rolling std\n",
        "        overall_df[f'{col}_roll_std_{label}'] = grouped[col].transform(lambda x: x.rolling(window, min_periods=1).std())\n",
        "\n",
        "# Ratios\n",
        "overall_df['water_gas_inj_ratio'] = overall_df['Water or Steam Injected (bbl)'] / (overall_df['Gas or Air Injected (Mcf)'] + 1e-6)\n",
        "\n",
        "overall_df['water_gas_prod_ratio'] = overall_df['Water Produced (bbl)'] / (overall_df['Gas Produced (Mcf)'] + 1e-6)\n",
        "overall_df['water_oil_prod_ratio'] = overall_df['Water Produced (bbl)'] / (overall_df['Oil Produced (bbl)'] + 1e-6)\n",
        "overall_df['gas_oil_prod_ratio'] = overall_df['Gas Produced (Mcf)'] / (overall_df['Oil Produced (bbl)'] + 1e-6) \n",
        "\n",
        "# Cumulative totals\n",
        "overall_df['cum_water_inj'] = grouped['Water or Steam Injected (bbl)'].cumsum()\n",
        "overall_df['cum_gas_inj'] = grouped['Gas or Air Injected (Mcf)'].cumsum()\n",
        "\n",
        "overall_df['cum_water_prod'] = grouped['Water Produced (bbl)'].cumsum()\n",
        "overall_df['cum_gas_prod'] = grouped['Gas Produced (Mcf)'].cumsum()\n",
        "overall_df['cum_oil_prod'] = grouped['Oil Produced (bbl)'].cumsum()\n",
        "\n",
        "# Additional features\n",
        "overall_df['inj_intensity'] = (overall_df['Water or Steam Injected (bbl)'] / (overall_df['Days Well Injected'] + 1e-6))\n",
        "overall_df['prod_efficiency'] = (overall_df['Oil Produced (bbl)'] / (overall_df['Oil Produced (bbl)'] + overall_df['Water Produced (bbl)'] + 1e-6))\n",
        "overall_df['pressure_diff'] = overall_df['Tubing Pressure'] - overall_df['Casing Pressure']\n",
        "overall_df['depth_norm_inj'] = (overall_df['Water or Steam Injected (bbl)'] / (overall_df['Depth'] + 1e-6))\n",
        "overall_df['btu_norm_gas'] = (overall_df['Gas Produced (Mcf)'] * overall_df['BTU'])\n",
        "\n",
        "# Seasonality\n",
        "overall_df['YearMonth'] = overall_df['YearMonth'].astype('period[M]')\n",
        "overall_df['Month'] = overall_df['YearMonth'].dt.month\n",
        "\n",
        "overall_df['month_sin'] = np.sin(2*np.pi*overall_df['Month']/12)\n",
        "overall_df['month_cos'] = np.cos(2*np.pi*overall_df['Month']/12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c910abeb",
      "metadata": {},
      "outputs": [],
      "source": [
        "overall_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c88df884",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create target cols\n",
        "overall_df['Total Earthquakes'] = (overall_df.groupby(['grid_id', 'YearMonth'])['Magnitude'].transform('count'))\n",
        "\n",
        "for label, window in time_windows.items():\n",
        "    overall_df[f'earthquake_count_{label}'] = (grouped['Total Earthquakes'].transform(lambda x: x.rolling(window, min_periods=1).sum()))\n",
        "\n",
        "    overall_df[f'earthquake_avg_mag_{label}'] = (grouped['Magnitude'].transform(lambda x: x.rolling(window, min_periods=1).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b7e030f",
      "metadata": {},
      "outputs": [],
      "source": [
        "overall_df.to_csv('data/final/final_dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e322b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/final/final_dataset.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6a38ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-val-test split: 80-10-10\n",
        "df = df.sort_values(\"YearMonth\").reset_index(drop=True) # To prevent leakage\n",
        "months = df['YearMonth'].dropna().sort_values().unique()\n",
        "n = len(months)\n",
        "train_months = months[:int(0.8*n)]\n",
        "val_months   = months[int(0.8*n):int(0.9*n)]\n",
        "test_months  = months[int(0.9*n):]\n",
        "\n",
        "train_df = df[df['YearMonth'].isin(train_months)]\n",
        "val_df   = df[df['YearMonth'].isin(val_months)]\n",
        "test_df  = df[df['YearMonth'].isin(test_months)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a0e70dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cols = list(df.columns)\n",
        "feature_cols = [c for c in df_cols[:-6] if c not in ['Production Date', 'YearMonth', 'grid_id', 'grid_geometry', 'Total Earthquakes', 'Gravity of Oil']]\n",
        "target_cols = df_cols[-6:]\n",
        "\n",
        "X_train = train_df[feature_cols].copy()\n",
        "X_val = val_df[feature_cols].copy()\n",
        "X_test = test_df[feature_cols].copy()\n",
        "\n",
        "# Handle categorical cols\n",
        "cat_cols = ['Source of Water', 'Kind of Water', 'Magnitude Type', 'Quality']\n",
        "X_train[cat_cols] = X_train[cat_cols].astype('category')\n",
        "X_val[cat_cols] = X_val[cat_cols].astype('category')\n",
        "X_test[cat_cols] = X_test[cat_cols].astype('category')\n",
        "\n",
        "count_targets = target_cols[::2]  # earthquake_count_3m,1y,5y\n",
        "mag_targets = target_cols[1::2] # earthquake_avg_mag_3m,1y,5y\n",
        "\n",
        "# earthquake_count_3m,1y,5y\n",
        "y_rate_train = np.log1p(train_df[count_targets])  \n",
        "y_rate_train = y_rate_train.clip(upper=y_rate_train.quantile(0.99), axis=1) # Clip outliers\n",
        "y_rate_val = np.log1p(val_df[count_targets])\n",
        "y_rate_test = np.log1p(test_df[count_targets])\n",
        "\n",
        "# earthquake_avg_mag_3m,1y,5y\n",
        "y_mag_train = train_df[target_cols[1::2]]  \n",
        "y_mag_val = val_df[target_cols[1::2]]\n",
        "y_mag_test = test_df[target_cols[1::2]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "069c1eec",
      "metadata": {},
      "source": [
        "## Model training, prediction and evaluation on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56252a6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train, eval and save results of the models\n",
        "os.makedirs('xgb_model', exist_ok=True)\n",
        "os.makedirs('xgb_model/results', exist_ok=True)\n",
        "os.makedirs('xgb_model/results/splits', exist_ok=True)\n",
        "os.makedirs('xgb_model/indiv', exist_ok=True)\n",
        "\n",
        "models = {}\n",
        "predictions = {}\n",
        "\n",
        "eval_file = 'xgb_model/results/final_eval.txt'\n",
        "with open(eval_file, 'w') as f:\n",
        "    f.write(\"Final Evaluation Metrics\\n\")\n",
        "\n",
        "all_targets = count_targets + mag_targets\n",
        "\n",
        "for target in all_targets:\n",
        "    # Determine if log-transform is needed\n",
        "    is_count = target in count_targets\n",
        "    y_train = y_rate_train[target] if is_count else y_mag_train[target]\n",
        "    y_val = y_rate_val[target] if is_count else y_mag_val[target]\n",
        "    \n",
        "    # Apply log-transform if count target\n",
        "    if is_count:\n",
        "        y_train_trans = np.log1p(y_train)\n",
        "        y_val_trans = np.log1p(y_val)\n",
        "    else:\n",
        "        y_train_trans = y_train\n",
        "        y_val_trans = y_val\n",
        "\n",
        "    # Initialize and fit model\n",
        "    model = XGBRegressor(\n",
        "        n_estimators=400,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=\"reg:squarederror\",\n",
        "        tree_method=\"hist\",\n",
        "        enable_categorical=True,\n",
        "        random_state=10\n",
        "    )\n",
        "    model.fit(X_train, y_train_trans, eval_set=[(X_val, y_val_trans)], verbose=0)\n",
        "    \n",
        "    # Predict\n",
        "    y_train_pred = np.expm1(model.predict(X_train)) if is_count else model.predict(X_train)\n",
        "    y_val_pred   = np.expm1(model.predict(X_val)) if is_count else model.predict(X_val)\n",
        "    y_test_pred  = np.expm1(model.predict(X_test)) if is_count else model.predict(X_test)\n",
        "    \n",
        "    # Store models and predictions\n",
        "    models[target] = model\n",
        "    predictions[target] = {'train': y_train_pred, 'val': y_val_pred, 'test': y_test_pred}\n",
        "    \n",
        "    # Save model\n",
        "    joblib.dump(model, f'xgb_model/indiv/{target}.pkl')\n",
        "    \n",
        "    # Save predictions\n",
        "    pd.DataFrame({'pred': y_train_pred}).to_csv(f'xgb_model/results/splits/{target}_train.csv', index=False)\n",
        "    pd.DataFrame({'pred': y_val_pred}).to_csv(f'xgb_model/results/splits/{target}_val.csv', index=False)\n",
        "    pd.DataFrame({'pred': y_test_pred}).to_csv(f'xgb_model/results/splits/{target}_test.csv', index=False)\n",
        "        \n",
        "    # Evaluate\n",
        "    rmse_val = mean_squared_error(y_val, y_val_pred)\n",
        "    r2_val = r2_score(y_val, y_val_pred)\n",
        "    print(f\"{target} RMSE: {rmse_val:.3f}, R^2: {r2_val:.3f}\")\n",
        "    \n",
        "    # Append evaluation metrics to txt file\n",
        "    with open(eval_file, 'a') as f:\n",
        "        f.write(f\"{target} RMSE: {rmse_val:.3f}, R^2: {r2_val:.3f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912f0d36",
      "metadata": {},
      "source": [
        "## SHAP analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf4bec7",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"vizs/shap\", exist_ok=True)\n",
        "os.makedirs(\"xgb_model/results\", exist_ok=True)\n",
        "os.makedirs(\"xgb_model/results/top10_features\", exist_ok=True)\n",
        "\n",
        "# Convert cat cols to cat codes\n",
        "X_train_shap = X_train.copy()\n",
        "for c in cat_cols:\n",
        "    X_train_shap[c] = X_train_shap[c].cat.codes\n",
        "\n",
        "top_features = {}\n",
        "\n",
        "for target, model in models.items():\n",
        "    print(f\"\\n=== SHAP Analysis for {target} ===\")\n",
        "    \n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X_train_shap)\n",
        "    \n",
        "    # Top 10 features\n",
        "    mean_abs_shap = pd.DataFrame({'feature': X_train_shap.columns,\n",
        "                                  'mean_abs_shap': np.abs(shap_values).mean(axis=0)\n",
        "                                }).sort_values(by='mean_abs_shap', ascending=False)\n",
        "    \n",
        "    top_10 = mean_abs_shap.head(10)\n",
        "    top_features[target] = top_10['feature'].tolist()\n",
        "    top_features_lst = top_10['feature'].tolist()\n",
        "    shap_values_top10 = shap_values[:, [X_train_shap.columns.get_loc(f) for f in top_features_lst]]\n",
        "    X_top10 = X_train_shap[top_features_lst]\n",
        "\n",
        "    # Save top 10 features to txt\n",
        "    top10_file = f\"xgb_model/results/top10_features/top10_{target}.txt\"\n",
        "    with open(top10_file, 'w') as f:\n",
        "        f.write(f\"Top 10 features for {target}:\\n\")\n",
        "        for feat in top_features[target]:\n",
        "            f.write(f\"{feat}\\n\")    \n",
        "    \n",
        "    # Barplot\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.barh(top_10['feature'][::-1], top_10['mean_abs_shap'][::-1], color='skyblue')\n",
        "    plt.xlabel(\"Mean |SHAP value|\")\n",
        "    plt.title(f\"Top 10 Features: {target}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"vizs/shap/{target}_barplot.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "    # Beeswarm for top 10 features\n",
        "    shap.summary_plot(shap_values_top10, X_top10, plot_type=\"dot\", show=False)\n",
        "    plt.title(f\"SHAP Beeswarm: {target}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"vizs/shap/{target}_beeswarm.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"Top 10 features for {target}:\")\n",
        "    print(top_features[target])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1afc4b21",
      "metadata": {},
      "source": [
        "## Predict on full dataset to create prediction maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4245ace3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction\n",
        "os.makedirs(\"xgb_model\", exist_ok=True)\n",
        "os.makedirs(\"xgb_model/results\", exist_ok=True)\n",
        "\n",
        "pred_df = df.copy()\n",
        "# Prepare full feature set \n",
        "X_full = df[feature_cols].copy()\n",
        "\n",
        "for c in X_full.select_dtypes(include='object').columns:\n",
        "    X_full[c] = X_full[c].astype('category')\n",
        "\n",
        "full_preds = {}\n",
        "\n",
        "# Earthquake rate preds\n",
        "for target in count_targets:\n",
        "    model = models[target]\n",
        "    \n",
        "    y_pred_log = model.predict(X_full)\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    \n",
        "    full_preds[target] = y_pred\n",
        "    pred_df['pred_' + target] = y_pred\n",
        "\n",
        "# Average earthquake magnitude preds\n",
        "for target in mag_targets:\n",
        "    model = models[target]\n",
        "\n",
        "    y_pred = model.predict(X_full)\n",
        "\n",
        "    full_preds[target] = y_pred\n",
        "    pred_df['pred_' + target] = y_pred\n",
        "\n",
        "# Save the combined predictions \n",
        "pred_df.to_csv(\"xgb_model/results/full_dataset_preds.csv\", index=False)\n",
        "\n",
        "# Save the full dataset model \n",
        "joblib.dump(models, \"xgb_model/full_dataset_xgb.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f33375",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction maps\n",
        "os.makedirs(\"vizs/pred_maps\", exist_ok=True)\n",
        "\n",
        "# Convert grid_geometry to shapely only if still string\n",
        "pred_df[\"grid_geometry\"] = pred_df[\"grid_geometry\"].apply(lambda x: wkt.loads(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Convert to GeoDataFrame\n",
        "gdf = gpd.GeoDataFrame(pred_df, geometry=\"grid_geometry\", crs=\"EPSG:4326\")\n",
        "\n",
        "# Aggregate preds by grid_id \n",
        "agg_gdf = gdf.groupby(\"grid_id\").agg({\"grid_geometry\": \"first\",\n",
        "                                      **{'pred_'+col: 'mean' for col in count_targets + mag_targets}\n",
        "                                    }).reset_index()\n",
        "\n",
        "agg_gdf = gpd.GeoDataFrame(agg_gdf, geometry='grid_geometry', crs='EPSG:4326')\n",
        "agg_gdf = agg_gdf.to_crs(epsg=3857)  # For basemap\n",
        "\n",
        "# Plot with LA basemap \n",
        "fig, ax = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "agg_gdf.plot(column='pred_'+count_targets[0], cmap='Reds', legend=True, alpha=0.6, edgecolor='k', ax=ax[0,0])\n",
        "ctx.add_basemap(ax[0,0], source=ctx.providers.CartoDB.Positron)\n",
        "ax[0,0].set_title(f'Predicted Earthquake Rate (3M)')\n",
        "\n",
        "agg_gdf.plot(column='pred_'+count_targets[1], cmap='Reds', legend=True, alpha=0.6, edgecolor='k', ax=ax[0,1])\n",
        "ctx.add_basemap(ax[0,1], source=ctx.providers.CartoDB.Positron)\n",
        "ax[0,1].set_title(f'Predicted Earthquake Rate (1Y)')\n",
        "\n",
        "agg_gdf.plot(column='pred_'+count_targets[2], cmap='Reds', legend=True, alpha=0.6, edgecolor='k', ax=ax[0,2])\n",
        "ctx.add_basemap(ax[0,2], source=ctx.providers.CartoDB.Positron)\n",
        "ax[0,2].set_title(f'Predicted Earthquake Rate (5Y)')\n",
        "\n",
        "agg_gdf.plot(column='pred_'+mag_targets[0], cmap='Blues', legend=True, alpha=0.6, edgecolor='k', ax=ax[1,0])\n",
        "ctx.add_basemap(ax[1,0], source=ctx.providers.CartoDB.Positron)\n",
        "ax[1,0].set_title(f'Predicted Average Earthquake Magnitude (3M)')\n",
        "\n",
        "agg_gdf.plot(column='pred_'+mag_targets[1], cmap='Blues', legend=True, alpha=0.6, edgecolor='k', ax=ax[1,1])\n",
        "ctx.add_basemap(ax[1,1], source=ctx.providers.CartoDB.Positron)\n",
        "ax[1,1].set_title(f'Predicted Average Earthquake Magnitude (1Y)')\n",
        "\n",
        "agg_gdf.plot(column='pred_'+mag_targets[2], cmap='Blues', legend=True, alpha=0.6, edgecolor='k', ax=ax[1,2])\n",
        "ctx.add_basemap(ax[1,2], source=ctx.providers.CartoDB.Positron)\n",
        "ax[1,2].set_title(f'Predicted Average Earthquake Magnitude (5Y)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('vizs/pred_maps/final_preds.png')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MoxjQuTUprIf",
        "fe11558a",
        "31781ef4",
        "DNUhPI5Dv8gJ",
        "a3afe24e",
        "537dd64e",
        "auHkM36W8-M0",
        "UkvjNdz59Ve2",
        "3B7C7wFgAHS1",
        "TYaQFhcM0UcL",
        "j93-M5CRCdHs",
        "-j4nkr274b0r",
        "2eddaf18",
        "ac7ca23a",
        "4bbf5471",
        "d1b856c2",
        "ab4b193c",
        "44185093",
        "82cb1f69",
        "994af407",
        "d9fd711f",
        "92be76c4",
        "fdde2e32",
        "bc3ca983",
        "58476a3f"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "shell_proj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
